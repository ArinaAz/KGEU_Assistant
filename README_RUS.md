# Отчёт по решению в репозитории

## Описание выбранной библиотеки

Для данного решения была выбрана библиотека transformers, разработанная командой Hugging Face. 
Transformers является одной из наиболее популярных библиотек для работы с моделями глубокого обучения в области обработки естественного языка (Natural Language Processing, NLP). 
Она предоставляет удобные и мощные инструменты для применения и дообучения предварительно обученных моделей NLP.

## Принцип работы модели и причины её выбора

В данном решении используется модель Helsinki-NLP/opus-mt-ru-en, предназначенная для машинного перевода с русского на английский язык. 
Эта модель основана на архитектуре Transformer, которая стала стандартом в области машинного перевода. Модель обучена на большом корпусе параллельных предложений на русском и английском языках.
Основной причиной выбора данной модели является её высокое качество перевода и широкая популярность в сообществе исследователей и разработчиков NLP.
Helsinki-NLP/opus-mt-ru-en демонстрирует высокую точность и хорошую способность к генерализации на различных типах текстов. Это делает её подходящим выбором для решения задачи машинного перевода.

## Структура датасета

Для обучения и оценки модели используется набор данных WMT16, содержащий параллельные предложения на русском и английском языках. 
Набор данных разделён на тренировочную, валидационную и тестовую выборки. В данном решении используется тренировочная выборка для обучения модели и валидационная выборка для оценки её качества.

## Особенности реализации алгоритма и его эффективность

Для запуска кода требуются необходимые библиотеки и поддержка драйыера CUDA.
Реализация алгоритма машинного перевода основана на библиотеке transformers и предоставляет удобные инструменты для обучения и оценки модели.
Процесс обработки данных и подготовки их для моделирования выполняется с использованием функции preprocess_function. Данная функция принимает входные примеры, извлекает их текстовые данные, добавляет префикс (если необходимо) и токенизирует их с использованием выбранного токенайзера. 
Затем она создает метки для обучения, используя токенайзер в режиме target_tokenizer.
Модель машинного перевода определена с использованием класса AutoModelForSeq2SeqLM из библиотеки transformers. Данный класс позволяет автоматически выбрать и загрузить соответствующую модель для задачи машинного перевода.
Для обучения модели используется класс Seq2SeqTrainer, который осуществляет обучение модели с использованием заданных аргументов и датасетов. Класс Seq2SeqTrainer также позволяет определить вычисление метрик для оценки качества модели.
Результаты обучения модели можно оценить с помощью вычисления метрики BLEU (Bilingual Evaluation Understudy), которая широко используется для оценки качества машинного перевода. В данном решении используется метрика sacreBLEU, предоставляемая библиотекой transformers.
Результаты обучения модели и оценки её качества могут быть сохранены и загружены в Hub Hugging Face с использованием аргументов класса Seq2SeqTrainingArguments.
Общая эффективность решения зависит от размера обучающей выборки, выбранной модели и параметров обучения. Для достижения лучших результатов рекомендуется провести тщательный подбор гиперпараметров и использовать большой объем размеченных данных для обучения модели.
 
## Ссылка на github:

https://github.com/ArinaAz/KGEU_Assistant